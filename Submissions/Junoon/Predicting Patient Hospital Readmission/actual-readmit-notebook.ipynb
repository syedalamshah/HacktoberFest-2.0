{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13439715,"sourceType":"datasetVersion","datasetId":8530482}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n\ntrain_path = \"/kaggle/input/dataset-4/diabetic_data.csv\"\nids_mapping_path = \"/kaggle/input/dataset-4/IDS_mapping.csv\"\n\n# LOAD\ndf_train = pd.read_csv(train_path)\ndf_ids = pd.read_csv(ids_mapping_path)\n\n# Quick look at the data\nprint(\"Train data shape:\", df_train.shape)\nprint(df_train.head())\nprint(\"\\nIDS mapping shape:\", df_ids.shape)\nprint(df_ids.head())\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:44:27.626006Z","iopub.execute_input":"2025-10-20T08:44:27.626694Z","iopub.status.idle":"2025-10-20T08:44:28.216512Z","shell.execute_reply.started":"2025-10-20T08:44:27.626667Z","shell.execute_reply":"2025-10-20T08:44:28.215780Z"}},"outputs":[{"name":"stdout","text":"Train data shape: (101766, 50)\n   encounter_id  patient_nbr             race  gender      age weight  \\\n0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n1        149190     55629189        Caucasian  Female  [10-20)      ?   \n2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n3        500364     82442376        Caucasian    Male  [30-40)      ?   \n4         16680     42519267        Caucasian    Male  [40-50)      ?   \n\n   admission_type_id  discharge_disposition_id  admission_source_id  \\\n0                  6                        25                    1   \n1                  1                         1                    7   \n2                  1                         1                    7   \n3                  1                         1                    7   \n4                  1                         1                    7   \n\n   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n0                 1  ...          No      No                   No   \n1                 3  ...          No      Up                   No   \n2                 2  ...          No      No                   No   \n3                 2  ...          No      Up                   No   \n4                 1  ...          No  Steady                   No   \n\n   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n0                   No                        No                       No   \n1                   No                        No                       No   \n2                   No                        No                       No   \n3                   No                        No                       No   \n4                   No                        No                       No   \n\n   metformin-pioglitazone  change diabetesMed readmitted  \n0                      No      No          No         NO  \n1                      No      Ch         Yes        >30  \n2                      No      No         Yes         NO  \n3                      No      Ch         Yes         NO  \n4                      No      Ch         Yes         NO  \n\n[5 rows x 50 columns]\n\nIDS mapping shape: (67, 2)\n  admission_type_id    description\n0                 1      Emergency\n1                 2         Urgent\n2                 3       Elective\n3                 4        Newborn\n4                 5  Not Available\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Handling Missing Values","metadata":{}},{"cell_type":"code","source":"\n# Replace '?' with NaN for missing value consistency\ndf_train.replace('?', np.nan, inplace=True)\ndf_ids.replace('?', np.nan, inplace=True)\n\n# -----------------------------\n# 1️⃣ Missing values in train\n# -----------------------------\nprint(\"=== Train Dataset Missing Values ===\")\nmissing_train = df_train.isnull().sum()\nmissing_train = missing_train[missing_train > 0].sort_values(ascending=False)\nprint(missing_train)\n\nprint(f\"\\nTotal rows: {df_train.shape[0]}, Total columns: {df_train.shape[1]}\")\nprint(f\"Number of duplicate rows in train: {df_train.duplicated().sum()}\")\n\n# -----------------------------\n# 2️⃣ Missing values in test/mapping\n# -----------------------------\nprint(\"\\n=== Test/Mapping Dataset Missing Values ===\")\nmissing_ids = df_ids.isnull().sum()\nmissing_ids = missing_ids[missing_ids > 0].sort_values(ascending=False)\nprint(missing_ids)\n\nprint(f\"\\nTotal rows: {df_ids.shape[0]}, Total columns: {df_ids.shape[1]}\")\nprint(f\"Number of duplicate rows in test/mapping: {df_ids.duplicated().sum()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:44:30.409045Z","iopub.execute_input":"2025-10-20T08:44:30.409342Z","iopub.status.idle":"2025-10-20T08:44:30.419807Z","shell.execute_reply.started":"2025-10-20T08:44:30.409320Z","shell.execute_reply":"2025-10-20T08:44:30.418919Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1447797404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Replace '?' with NaN for missing value consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"],"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# Drop columns with very high missing values\ncols_to_drop = ['weight', 'max_glu_serum', 'A1Cresult', 'medical_specialty', 'payer_code']\ndf_train.drop(columns=cols_to_drop, inplace=True)\n\n# For remaining columns with few missing values, fill with 'Unknown' (categorical) or 0 (numeric)\ncat_cols_fill = ['race', 'diag_1', 'diag_2', 'diag_3']\nfor col in cat_cols_fill:\n    df_train[col] = df_train[col].fillna('Unknown')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T10:54:45.636887Z","iopub.execute_input":"2025-10-20T10:54:45.637375Z","iopub.status.idle":"2025-10-20T10:54:45.655089Z","shell.execute_reply.started":"2025-10-20T10:54:45.637353Z","shell.execute_reply":"2025-10-20T10:54:45.654193Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1630763729.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drop columns with very high missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcols_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_glu_serum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A1Cresult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'medical_specialty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'payer_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# For remaining columns with few missing values, fill with 'Unknown' (categorical) or 0 (numeric)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['weight', 'max_glu_serum', 'A1Cresult', 'medical_specialty', 'payer_code'] not found in axis\""],"ename":"KeyError","evalue":"\"['weight', 'max_glu_serum', 'A1Cresult', 'medical_specialty', 'payer_code'] not found in axis\"","output_type":"error"}],"execution_count":58},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# 1️⃣ Missing values in train (after dropping/filling)\n# -----------------------------\nprint(\"=== Train Dataset Missing Values (After Cleaning) ===\")\nmissing_train = df_train.isnull().sum()\nmissing_train = missing_train[missing_train > 0].sort_values(ascending=False)\nif missing_train.empty:\n    print(\"No missing values in train dataset ✅\")\nelse:\n    print(missing_train)\n\n# -----------------------------\n# 2️⃣ Missing values in test/mapping (after filling)\n# -----------------------------\nprint(\"\\n=== Test/Mapping Dataset Missing Values (After Cleaning) ===\")\nmissing_ids = df_ids.isnull().sum()\nmissing_ids = missing_ids[missing_ids > 0].sort_values(ascending=False)\nif missing_ids.empty:\n    print(\"No missing values in test/mapping dataset ✅\")\nelse:\n    print(missing_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:44:49.616452Z","iopub.execute_input":"2025-10-20T08:44:49.617107Z","iopub.status.idle":"2025-10-20T08:44:49.778336Z","shell.execute_reply.started":"2025-10-20T08:44:49.617083Z","shell.execute_reply":"2025-10-20T08:44:49.777725Z"}},"outputs":[{"name":"stdout","text":"=== Train Dataset Missing Values (After Cleaning) ===\nNo missing values in train dataset ✅\n\n=== Test/Mapping Dataset Missing Values (After Cleaning) ===\ndescription          5\nadmission_type_id    2\ndtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Clean and convert target\ndf_train['readmitted'] = df_train['readmitted'].str.strip()  # remove extra spaces\ndf_train['readmitted'] = df_train['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\ndf_train['readmitted'] = df_train['readmitted'].astype(int)\n\n# Check the conversion\nprint(\"Readmission counts after fixing target:\")\nprint(df_train['readmitted'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:44:53.849565Z","iopub.execute_input":"2025-10-20T08:44:53.850331Z","iopub.status.idle":"2025-10-20T08:44:53.914028Z","shell.execute_reply.started":"2025-10-20T08:44:53.850293Z","shell.execute_reply":"2025-10-20T08:44:53.913244Z"}},"outputs":[{"name":"stdout","text":"Readmission counts after fixing target:\nreadmitted\n0    90409\n1    11357\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# -----------------------------\n# Numeric features summary by readmission\n# -----------------------------\nnum_features = ['num_lab_procedures', 'num_procedures', 'num_medications', \n                'number_outpatient', 'number_emergency', 'number_inpatient', \n                'time_in_hospital', 'number_diagnoses']\n\nprint(\"\\n--- Numeric Features Summary by Readmission ---\")\nfor col in num_features:\n    summary = df_train.groupby('readmitted')[col].describe()\n    print(f\"\\nFeature: {col}\")\n    print(summary)\n\n# Mean comparison for quick insights\nprint(\"\\n--- Mean of numeric features by readmission ---\")\nprint(df_train.groupby('readmitted')[num_features].mean())\n\n# -----------------------------\n# Categorical features readmission rate\n# -----------------------------\ncat_features = ['race', 'gender', 'age', 'change', 'diabetesMed']\n\nprint(\"\\n--- Categorical Features Readmission Rate ---\")\nfor col in cat_features:\n    cross_tab = pd.crosstab(df_train[col], df_train['readmitted'], normalize='index')\n    readmit_rate_col = cross_tab.get(1, pd.Series(0, index=cross_tab.index))\n    print(f\"\\nReadmission rate by {col}:\")\n    print(readmit_rate_col.sort_values(ascending=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:44:55.709117Z","iopub.execute_input":"2025-10-20T08:44:55.709405Z","iopub.status.idle":"2025-10-20T08:44:55.916234Z","shell.execute_reply.started":"2025-10-20T08:44:55.709385Z","shell.execute_reply":"2025-10-20T08:44:55.915511Z"}},"outputs":[{"name":"stdout","text":"\n--- Numeric Features Summary by Readmission ---\n\nFeature: num_lab_procedures\n              count       mean        std  min   25%   50%   75%    max\nreadmitted                                                             \n0           90409.0  42.953644  19.719348  1.0  31.0  44.0  57.0  129.0\n1           11357.0  44.226028  19.276087  1.0  33.0  45.0  58.0  132.0\n\nFeature: num_procedures\n              count      mean       std  min  25%  50%  75%  max\nreadmitted                                                      \n0           90409.0  1.347123  1.714242  0.0  0.0  1.0  2.0  6.0\n1           11357.0  1.280884  1.635992  0.0  0.0  1.0  2.0  6.0\n\nFeature: num_medications\n              count       mean       std  min   25%   50%   75%   max\nreadmitted                                                           \n0           90409.0  15.911137  8.124725  1.0  10.0  15.0  20.0  79.0\n1           11357.0  16.903143  8.096696  1.0  11.0  16.0  21.0  81.0\n\nFeature: number_outpatient\n              count      mean       std  min  25%  50%  75%   max\nreadmitted                                                       \n0           90409.0  0.360871  1.262484  0.0  0.0  0.0  0.0  42.0\n1           11357.0  0.436911  1.302788  0.0  0.0  0.0  0.0  40.0\n\nFeature: number_emergency\n              count      mean       std  min  25%  50%  75%   max\nreadmitted                                                       \n0           90409.0  0.177803  0.857353  0.0  0.0  0.0  0.0  76.0\n1           11357.0  0.357313  1.370384  0.0  0.0  0.0  0.0  64.0\n\nFeature: number_inpatient\n              count      mean       std  min  25%  50%  75%   max\nreadmitted                                                       \n0           90409.0  0.561648  1.125315  0.0  0.0  0.0  1.0  19.0\n1           11357.0  1.224003  1.954577  0.0  0.0  0.0  2.0  21.0\n\nFeature: time_in_hospital\n              count      mean       std  min  25%  50%  75%   max\nreadmitted                                                       \n0           90409.0  4.349224  2.976382  1.0  2.0  4.0  6.0  14.0\n1           11357.0  4.768249  3.028165  1.0  2.0  4.0  6.0  14.0\n\nFeature: number_diagnoses\n              count      mean       std  min  25%  50%  75%   max\nreadmitted                                                       \n0           90409.0  7.388667  1.950151  1.0  6.0  8.0  9.0  16.0\n1           11357.0  7.692789  1.773477  1.0  6.0  9.0  9.0  16.0\n\n--- Mean of numeric features by readmission ---\n            num_lab_procedures  num_procedures  num_medications  \\\nreadmitted                                                        \n0                    42.953644        1.347123        15.911137   \n1                    44.226028        1.280884        16.903143   \n\n            number_outpatient  number_emergency  number_inpatient  \\\nreadmitted                                                          \n0                    0.360871          0.177803          0.561648   \n1                    0.436911          0.357313          1.224003   \n\n            time_in_hospital  number_diagnoses  \nreadmitted                                      \n0                   4.349224          7.388667  \n1                   4.768249          7.692789  \n\n--- Categorical Features Readmission Rate ---\n\nReadmission rate by race:\nrace\nCaucasian          0.112906\nAfricanAmerican    0.112181\nHispanic           0.104075\nAsian              0.101404\nOther              0.096282\n?                  0.082710\nName: 1, dtype: float64\n\nReadmission rate by gender:\ngender\nFemale             0.112452\nMale               0.110615\nUnknown/Invalid    0.000000\nName: 1, dtype: float64\n\nReadmission rate by age:\nage\n[20-30)     0.142426\n[80-90)     0.120835\n[70-80)     0.117731\n[30-40)     0.112318\n[60-70)     0.111284\n[90-100)    0.110992\n[40-50)     0.106040\n[50-60)     0.096662\n[10-20)     0.057887\n[0-10)      0.018634\nName: 1, dtype: float64\n\nReadmission rate by change:\nchange\nCh    0.118228\nNo    0.105908\nName: 1, dtype: float64\n\nReadmission rate by diabetesMed:\ndiabetesMed\nYes    0.116267\nNo     0.095971\nName: 1, dtype: float64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# CORRELATIONS","metadata":{}},{"cell_type":"code","source":"# Compute correlations with target\nnumeric_features = ['num_lab_procedures', 'num_procedures', 'num_medications', \n                    'number_outpatient', 'number_emergency', 'number_inpatient', \n                    'time_in_hospital', 'number_diagnoses']\n\ncorrelations = df_train[numeric_features + ['readmitted']].corr()['readmitted'].sort_values(ascending=False)\nprint(\"\\n--- Correlation with Readmission ---\")\nprint(correlations)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:44:59.450174Z","iopub.execute_input":"2025-10-20T08:44:59.450849Z","iopub.status.idle":"2025-10-20T08:44:59.488257Z","shell.execute_reply.started":"2025-10-20T08:44:59.450823Z","shell.execute_reply":"2025-10-20T08:44:59.487638Z"}},"outputs":[{"name":"stdout","text":"\n--- Correlation with Readmission ---\nreadmitted            1.000000\nnumber_inpatient      0.165147\nnumber_emergency      0.060747\nnumber_diagnoses      0.049524\ntime_in_hospital      0.044199\nnum_medications       0.038432\nnum_lab_procedures    0.020364\nnumber_outpatient     0.018893\nnum_procedures       -0.012227\nName: readmitted, dtype: float64\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# List of categorical features\ncat_features = ['race', 'gender', 'age', 'change', 'diabetesMed']\n\n# Encode each categorical feature\nfor col in cat_features:\n    df_train[col] = df_train[col].astype(str)  # ensure string type\n    le = LabelEncoder()\n    df_train[col] = le.fit_transform(df_train[col])\n\n# Check a few rows\nprint(df_train[cat_features + ['readmitted']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:02.188354Z","iopub.execute_input":"2025-10-20T08:45:02.189118Z","iopub.status.idle":"2025-10-20T08:45:02.794385Z","shell.execute_reply.started":"2025-10-20T08:45:02.189093Z","shell.execute_reply":"2025-10-20T08:45:02.793667Z"}},"outputs":[{"name":"stdout","text":"   race  gender  age  change  diabetesMed  readmitted\n0     3       0    0       1            0           0\n1     3       0    1       0            1           0\n2     1       0    2       1            1           0\n3     3       1    3       0            1           0\n4     3       1    4       0            1           0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Outliers","metadata":{}},{"cell_type":"markdown","source":"2️⃣ Interpretation\n\nSkewness > 0.5 → right-skewed (long tail to the right, outliers are high)\n\nSkewness < -0.5 → left-skewed (long tail to the left, outliers are low)\n\n-0.5 ≤ skewness ≤ 0.5 → approximately symmetric","metadata":{}},{"cell_type":"code","source":"# Numeric features to check\nnumeric_features = ['num_lab_procedures', 'num_procedures', 'num_medications', \n                    'number_outpatient', 'number_emergency', 'number_inpatient', \n                    'time_in_hospital', 'number_diagnoses']\n\n# Compute skewness\nskew_values = df_train[numeric_features].skew()\nprint(\"--- Skewness of Numeric Features ---\")\nprint(skew_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:25.352834Z","iopub.execute_input":"2025-10-20T08:45:25.353215Z","iopub.status.idle":"2025-10-20T08:45:25.382753Z","shell.execute_reply.started":"2025-10-20T08:45:25.353194Z","shell.execute_reply":"2025-10-20T08:45:25.382016Z"}},"outputs":[{"name":"stdout","text":"--- Skewness of Numeric Features ---\nnum_lab_procedures    -0.236544\nnum_procedures         1.316415\nnum_medications        1.326672\nnumber_outpatient      8.832959\nnumber_emergency      22.855582\nnumber_inpatient       3.614139\ntime_in_hospital       1.133999\nnumber_diagnoses      -0.876746\ndtype: float64\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Handling missing values","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimport pandas as pd\n\n# Split column types\nnumeric_cols = X_encoded.select_dtypes(include=['int64','float64']).columns\ncategorical_cols = X_encoded.select_dtypes(include=['object']).columns\nboolean_cols = X_encoded.select_dtypes(include=['bool']).columns\n\n# --- 1. Numeric ---\nif numeric_cols.any():\n    num_imputer = SimpleImputer(strategy='median')\n    X_encoded[numeric_cols] = pd.DataFrame(\n        num_imputer.fit_transform(X_encoded[numeric_cols]),\n        columns=numeric_cols,\n        index=X_encoded.index\n    )\n\n# --- 2. Categorical ---\nif categorical_cols.any():\n    cat_imputer = SimpleImputer(strategy='most_frequent')\n    X_encoded[categorical_cols] = pd.DataFrame(\n        cat_imputer.fit_transform(X_encoded[categorical_cols]),\n        columns=categorical_cols,\n        index=X_encoded.index\n    )\n\n# --- 3. Boolean ---\nif boolean_cols.any():\n    bool_imputer = SimpleImputer(strategy='most_frequent')\n    X_encoded[boolean_cols] = pd.DataFrame(\n        bool_imputer.fit_transform(X_encoded[boolean_cols].astype(int)),\n        columns=boolean_cols,\n        index=X_encoded.index\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:28.448045Z","iopub.execute_input":"2025-10-20T08:45:28.448331Z","iopub.status.idle":"2025-10-20T08:45:28.739337Z","shell.execute_reply.started":"2025-10-20T08:45:28.448311Z","shell.execute_reply":"2025-10-20T08:45:28.738221Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1935684492.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Split column types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnumeric_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcategorical_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mboolean_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_encoded' is not defined"],"ename":"NameError","evalue":"name 'X_encoded' is not defined","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.impute import SimpleImputer\n\n# X_encoded is your features DataFrame\n\n# --- 1. Identify column types properly ---\nnumeric_cols = X_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_cols = X_encoded.select_dtypes(include=['object']).columns.tolist()\nboolean_cols = X_encoded.select_dtypes(include=['bool']).columns.tolist()\n\nprint(f\"Numeric: {len(numeric_cols)}, Categorical: {len(categorical_cols)}, Boolean: {len(boolean_cols)}\")\n\n# --- 2. Impute numeric columns (median) ---\nif numeric_cols:\n    num_imputer = SimpleImputer(strategy='median')\n    X_encoded[numeric_cols] = num_imputer.fit_transform(X_encoded[numeric_cols])\n\n# --- 3. Impute categorical columns (most frequent) ---\nif categorical_cols:\n    cat_imputer = SimpleImputer(strategy='most_frequent')\n    X_encoded[categorical_cols] = cat_imputer.fit_transform(X_encoded[categorical_cols])\n\n# --- 4. Impute boolean columns (most frequent) ---\nif boolean_cols:\n    bool_imputer = SimpleImputer(strategy='most_frequent')\n    # Convert boolean to object before imputing to avoid errors\n    X_encoded[boolean_cols] = bool_imputer.fit_transform(X_encoded[boolean_cols].astype(object))\n\n# --- 5. Final check ---\ntotal_nans = X_encoded.isna().sum().sum()\nprint(f\"Total NaNs after imputation: {total_nans}\")  # Should be 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:40.475174Z","iopub.execute_input":"2025-10-20T08:45:40.475833Z","iopub.status.idle":"2025-10-20T08:45:40.492584Z","shell.execute_reply.started":"2025-10-20T08:45:40.475801Z","shell.execute_reply":"2025-10-20T08:45:40.490929Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1556515321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# --- 1. Identify column types properly ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnumeric_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float64'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcategorical_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mboolean_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_encoded' is not defined"],"ename":"NameError","evalue":"name 'X_encoded' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"# Train/Test SPLIT","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, recall_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:43.336729Z","iopub.execute_input":"2025-10-20T08:45:43.337539Z","iopub.status.idle":"2025-10-20T08:45:43.341568Z","shell.execute_reply.started":"2025-10-20T08:45:43.337504Z","shell.execute_reply":"2025-10-20T08:45:43.340631Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"X = df_train.drop('readmitted', axis=1)\ny = df_train['readmitted']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:45.914102Z","iopub.execute_input":"2025-10-20T08:45:45.914722Z","iopub.status.idle":"2025-10-20T08:45:45.938928Z","shell.execute_reply.started":"2025-10-20T08:45:45.914698Z","shell.execute_reply":"2025-10-20T08:45:45.938276Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Stratified 5-fold cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Example loop to show how to split\nfor fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n    \n    # Just printing counts for demonstration\n    print(f\"Fold {fold}:\")\n    print(\"Train class distribution:\", y_train_fold.value_counts(normalize=True).to_dict())\n    print(\"Validation class distribution:\", y_val_fold.value_counts(normalize=True).to_dict())\n    print(\"-\"*40)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:47.628963Z","iopub.execute_input":"2025-10-20T08:45:47.629564Z","iopub.status.idle":"2025-10-20T08:45:47.826664Z","shell.execute_reply.started":"2025-10-20T08:45:47.629539Z","shell.execute_reply":"2025-10-20T08:45:47.825652Z"}},"outputs":[{"name":"stdout","text":"Fold 1:\nTrain class distribution: {0: 0.8884071144303051, 1: 0.11159288556969489}\nValidation class distribution: {0: 0.888375749238479, 1: 0.11162425076152108}\n----------------------------------------\nFold 2:\nTrain class distribution: {0: 0.8883962020807488, 1: 0.11160379791925122}\nValidation class distribution: {0: 0.8884193976317988, 1: 0.11158060236820125}\n----------------------------------------\nFold 3:\nTrain class distribution: {0: 0.8883962020807488, 1: 0.11160379791925122}\nValidation class distribution: {0: 0.8884193976317988, 1: 0.11158060236820125}\n----------------------------------------\nFold 4:\nTrain class distribution: {0: 0.8883962020807488, 1: 0.11160379791925122}\nValidation class distribution: {0: 0.8884193976317988, 1: 0.11158060236820125}\n----------------------------------------\nFold 5:\nTrain class distribution: {0: 0.8884084851313673, 1: 0.11159151486863278}\nValidation class distribution: {0: 0.8883702648258242, 1: 0.11162973517417579}\n----------------------------------------\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"**Checking is there any NAN left**","metadata":{}},{"cell_type":"code","source":"# Ensure all categorical features are numeric\nX_final = X_selected.copy()\n\nfor col in X_final.columns:\n    if X_final[col].dtype == 'object' or X_final[col].dtype == 'bool':\n        X_final[col] = pd.factorize(X_final[col])[0]\n\n# Confirm no NaNs\nprint(\"Total NaNs:\", X_final.isna().sum().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:32:52.847148Z","iopub.execute_input":"2025-10-20T09:32:52.847949Z","iopub.status.idle":"2025-10-20T09:32:52.863752Z","shell.execute_reply.started":"2025-10-20T09:32:52.847922Z","shell.execute_reply":"2025-10-20T09:32:52.862912Z"}},"outputs":[{"name":"stdout","text":"Total NaNs: 0\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# SMOTE","metadata":{}},{"cell_type":"code","source":"!pip install imbalanced-learn==0.10.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:50.975352Z","iopub.execute_input":"2025-10-20T08:45:50.976124Z","iopub.status.idle":"2025-10-20T08:45:56.146356Z","shell.execute_reply.started":"2025-10-20T08:45:50.976098Z","shell.execute_reply":"2025-10-20T08:45:56.145323Z"}},"outputs":[{"name":"stdout","text":"Collecting imbalanced-learn==0.10.1\n  Downloading imbalanced_learn-0.10.1-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.10.1) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.10.1) (1.15.3)\nRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.10.1) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.10.1) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.10.1) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.10.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.10.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.10.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.10.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.10.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.10.1) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn==0.10.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn==0.10.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->imbalanced-learn==0.10.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->imbalanced-learn==0.10.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->imbalanced-learn==0.10.1) (2024.2.0)\nDownloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: imbalanced-learn\n  Attempting uninstall: imbalanced-learn\n    Found existing installation: imbalanced-learn 0.13.0\n    Uninstalling imbalanced-learn-0.13.0:\n      Successfully uninstalled imbalanced-learn-0.13.0\nSuccessfully installed imbalanced-learn-0.10.1\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!pip install scikit-learn==1.2.2 imbalanced-learn==0.11.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:56.148200Z","iopub.execute_input":"2025-10-20T08:45:56.148526Z","iopub.status.idle":"2025-10-20T08:45:59.947755Z","shell.execute_reply.started":"2025-10-20T08:45:56.148503Z","shell.execute_reply":"2025-10-20T08:45:59.946993Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting imbalanced-learn==0.11.0\n  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nDownloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: imbalanced-learn\n  Attempting uninstall: imbalanced-learn\n    Found existing installation: imbalanced-learn 0.10.1\n    Uninstalling imbalanced-learn-0.10.1:\n      Successfully uninstalled imbalanced-learn-0.10.1\nSuccessfully installed imbalanced-learn-0.11.0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:45:59.948854Z","iopub.execute_input":"2025-10-20T08:45:59.949165Z","iopub.status.idle":"2025-10-20T08:46:00.222789Z","shell.execute_reply.started":"2025-10-20T08:45:59.949134Z","shell.execute_reply":"2025-10-20T08:46:00.222190Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Convert all categorical columns to numeric (if not done yet)\nX_numeric = pd.get_dummies(X, drop_first=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:46:00.224376Z","iopub.execute_input":"2025-10-20T08:46:00.224955Z","iopub.status.idle":"2025-10-20T08:46:00.718433Z","shell.execute_reply.started":"2025-10-20T08:46:00.224934Z","shell.execute_reply":"2025-10-20T08:46:00.717781Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_numeric, y), 1):\n    X_train_fold, X_val_fold = X_numeric.iloc[train_idx], X_numeric.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Apply SMOTE\n    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n    \n    # Now X_train_res is fully numeric and SMOTE will work\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:46:02.846222Z","iopub.execute_input":"2025-10-20T08:46:02.846942Z","iopub.status.idle":"2025-10-20T08:46:46.572501Z","shell.execute_reply.started":"2025-10-20T08:46:02.846916Z","shell.execute_reply":"2025-10-20T08:46:46.571715Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Encode all categorical columns upfront\nX_encoded = pd.get_dummies(X, drop_first=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:46:46.573727Z","iopub.execute_input":"2025-10-20T08:46:46.574361Z","iopub.status.idle":"2025-10-20T08:46:47.050033Z","shell.execute_reply.started":"2025-10-20T08:46:46.574340Z","shell.execute_reply":"2025-10-20T08:46:47.049302Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import xgboost as xgb\n\nclf = xgb.XGBClassifier(\n    n_estimators=300,\n    scale_pos_weight=(len(y_train_fold)-sum(y_train_fold))/sum(y_train_fold),\n    random_state=42,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:46:47.050906Z","iopub.execute_input":"2025-10-20T08:46:47.051212Z","iopub.status.idle":"2025-10-20T08:46:47.318666Z","shell.execute_reply.started":"2025-10-20T08:46:47.051186Z","shell.execute_reply":"2025-10-20T08:46:47.318110Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import f1_score, recall_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import StratifiedKFold\n\n# Already have X_encoded and y\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_encoded, y), 1):\n    X_train_fold = X_encoded.iloc[train_idx]\n    X_val_fold = X_encoded.iloc[val_idx]\n\n    y_train_fold = y.iloc[train_idx]\n    y_val_fold = y.iloc[val_idx]\n    \n    # SMOTE on training fold\n    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n    \n    # Convert back to DataFrame and align columns\n    X_train_res = pd.DataFrame(X_train_res, columns=X_train_fold.columns)\n    \n    # Initialize XGBoost classifier with imbalance handling\n    clf = xgb.XGBClassifier(\n        n_estimators=300,\n        scale_pos_weight=(len(y_train_fold)-sum(y_train_fold))/sum(y_train_fold),\n        use_label_encoder=False,\n        eval_metric='logloss',\n        tree_method='gpu_hist',  # use GPU\n        random_state=42\n    )\n    \n    clf.fit(X_train_res, y_train_res)\n    \n    # Predict on validation (columns already aligned)\n    y_pred = clf.predict(X_val_fold)\n    \n    f1 = f1_score(y_val_fold, y_pred)\n    recall = recall_score(y_val_fold, y_pred)\n    print(f\"Fold {fold}: F1 = {f1:.4f}, Recall = {recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:46:47.320337Z","iopub.execute_input":"2025-10-20T08:46:47.320565Z","iopub.status.idle":"2025-10-20T08:50:19.296065Z","shell.execute_reply.started":"2025-10-20T08:46:47.320549Z","shell.execute_reply":"2025-10-20T08:50:19.295184Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:47:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:47:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:47:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1: F1 = 0.2408, Recall = 0.4555\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:47:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:48:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 2: F1 = 0.2520, Recall = 0.4817\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:48:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:48:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 3: F1 = 0.2429, Recall = 0.4509\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:49:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:49:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 4: F1 = 0.2401, Recall = 0.4412\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:50:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:50:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 5: F1 = 0.2433, Recall = 0.4494\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"**BY FILTER METHOD**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\n# Remove features with very low variance (almost constant)\nselector = VarianceThreshold(threshold=0.01)  # Keep features with >1% variance\nX_filtered = selector.fit_transform(X_encoded)\n\n# Keep column names\nX_filtered = pd.DataFrame(X_filtered, columns=X_encoded.columns[selector.get_support()])\nprint(\"Remaining features after low-variance filter:\", X_filtered.shape[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:50:19.297194Z","iopub.execute_input":"2025-10-20T08:50:19.297499Z","iopub.status.idle":"2025-10-20T08:50:24.296165Z","shell.execute_reply.started":"2025-10-20T08:50:19.297480Z","shell.execute_reply":"2025-10-20T08:50:24.295442Z"}},"outputs":[{"name":"stdout","text":"Remaining features after low-variance filter: 103\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"**BY EMBEDDED METHOD**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\n\n# Train a Random Forest on the filtered features\nrf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\nrf.fit(X_filtered, y)\n\n# Get feature importances\nimportances = pd.Series(rf.feature_importances_, index=X_filtered.columns)\nimportances = importances.sort_values(ascending=False)\n\n# Show top 20 features\nprint(\"Top 20 features by importance:\")\nprint(importances.head(20))\n\n# Optionally, select features above a threshold\nthreshold = 0.005  # keep features with importance > 0.5%\ntop_features = importances[importances > threshold].index\nX_selected = X_filtered[top_features]\nprint(\"Features selected for modeling:\", X_selected.shape[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:50:24.296976Z","iopub.execute_input":"2025-10-20T08:50:24.297267Z","iopub.status.idle":"2025-10-20T08:50:44.516051Z","shell.execute_reply.started":"2025-10-20T08:50:24.297248Z","shell.execute_reply":"2025-10-20T08:50:44.515361Z"}},"outputs":[{"name":"stdout","text":"Top 20 features by importance:\nencounter_id                0.089830\npatient_nbr                 0.088401\nnum_lab_procedures          0.075616\nnum_medications             0.066207\ntime_in_hospital            0.047171\nnumber_inpatient            0.041132\nage                         0.040743\nnum_procedures              0.033314\ndischarge_disposition_id    0.033051\nnumber_diagnoses            0.030588\nadmission_type_id           0.024635\nadmission_source_id         0.021196\nrace                        0.019513\ngender                      0.016918\nnumber_outpatient           0.016511\nnumber_emergency            0.014749\nchange                      0.010869\ninsulin_Steady              0.010480\ninsulin_No                  0.008445\ndiag_2_276                  0.007403\ndtype: float64\nFeatures selected for modeling: 37\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"------------------------------------","metadata":{}},{"cell_type":"markdown","source":"**DROPPEDE THE IDENTIFIERS**","metadata":{}},{"cell_type":"code","source":"# Drop identifiers\nX_filtered = X_encoded.drop(columns=['encounter_id', 'patient_nbr'], errors='ignore')\n\n(excluding IDs)\ntop_features = [\n    'num_lab_procedures', 'num_medications', 'time_in_hospital', 'number_inpatient',\n    'age', 'num_procedures', 'discharge_disposition_id', 'number_diagnoses',\n    'admission_type_id', 'admission_source_id', 'race', 'gender',\n    'number_outpatient', 'number_emergency', 'change',\n    'insulin_Steady', 'insulin_No', 'diag_2_276'\n]\n\n# Filter to keep only top predictive features\nX_filtered = X_filtered[top_features]\n\nprint(f\"Remaining features for modeling: {X_filtered.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:50:44.516800Z","iopub.execute_input":"2025-10-20T08:50:44.517087Z","iopub.status.idle":"2025-10-20T08:50:44.598839Z","shell.execute_reply.started":"2025-10-20T08:50:44.517062Z","shell.execute_reply":"2025-10-20T08:50:44.598111Z"}},"outputs":[{"name":"stdout","text":"Remaining features for modeling: 18\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# ENCODING","metadata":{}},{"cell_type":"code","source":"# One-hot encode insulin and diag_2\ndf_encoded = pd.get_dummies(df_train, columns=['insulin', 'diag_2'], drop_first=False)\n\n# Now we can safely select top features\nX_selected = df_encoded[top_features].copy()\ny = df_encoded['readmitted']\n\n# Check\nprint(\"Columns available in X_selected:\")\nprint(X_selected.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:11:28.803918Z","iopub.execute_input":"2025-10-20T09:11:28.804636Z","iopub.status.idle":"2025-10-20T09:11:28.967099Z","shell.execute_reply.started":"2025-10-20T09:11:28.804615Z","shell.execute_reply":"2025-10-20T09:11:28.966287Z"}},"outputs":[{"name":"stdout","text":"Columns available in X_selected:\nIndex(['num_lab_procedures', 'num_medications', 'time_in_hospital',\n       'number_inpatient', 'age', 'num_procedures', 'discharge_disposition_id',\n       'number_diagnoses', 'admission_type_id', 'admission_source_id', 'race',\n       'gender', 'number_outpatient', 'number_emergency', 'change',\n       'insulin_Steady', 'insulin_No', 'diag_2_276'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, recall_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\n\n# X_selected already contains only the top features\n# y is the target\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y), 1):\n    X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Apply SMOTE only on training fold\n    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n    \n    # Convert back to DataFrame to keep column names aligned\n    X_train_res = pd.DataFrame(X_train_res, columns=X_train_fold.columns)\n    \n    # Initialize XGBoost classifier with GPU\n    clf = xgb.XGBClassifier(\n        n_estimators=300,\n        scale_pos_weight=(len(y_train_fold) - sum(y_train_fold)) / sum(y_train_fold),\n        use_label_encoder=False,\n        eval_metric='logloss',\n        tree_method='gpu_hist',\n        random_state=42\n    )\n    \n    clf.fit(X_train_res, y_train_res)\n    \n    # Predict on validation fold\n    y_pred = clf.predict(X_val_fold)\n    \n    # Evaluate\n    f1 = f1_score(y_val_fold, y_pred)\n    recall = recall_score(y_val_fold, y_pred)\n    print(f\"Fold {fold}: F1 = {f1:.4f}, Recall = {recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:12:19.771215Z","iopub.execute_input":"2025-10-20T09:12:19.771729Z","iopub.status.idle":"2025-10-20T09:12:34.203843Z","shell.execute_reply.started":"2025-10-20T09:12:19.771705Z","shell.execute_reply":"2025-10-20T09:12:34.202855Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1: F1 = 0.2209, Recall = 0.6364\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 2: F1 = 0.2195, Recall = 0.6420\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 3: F1 = 0.2204, Recall = 0.6354\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 4: F1 = 0.2206, Recall = 0.6398\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 5: F1 = 0.2139, Recall = 0.6215\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:12:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# Hyper Parameter tunning\n","metadata":{}},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\nimport numpy as np\n\n# Assume X_selected and y are already defined\nX = X_selected.copy()\ny = y.copy()\n\n# Stratified K-Fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ndef objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n        \"tree_method\": \"hist\",\n        \"device\": \"cuda\",\n        \"eval_metric\": \"logloss\",\n        \"use_label_encoder\": False,\n        \"random_state\": 42,\n        \"scale_pos_weight\": (len(y) - sum(y)) / sum(y)  # handle imbalance\n    }\n\n    f1_scores = []\n\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Apply SMOTE on training fold\n        smote = SMOTE(sampling_strategy=0.5, random_state=42)\n        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\n        # Train XGBoost\n        clf = xgb.XGBClassifier(**params)\n        clf.fit(X_train_res, y_train_res)\n\n        # Predict on validation fold\n        y_pred = clf.predict(X_val)\n        f1_scores.append(f1_score(y_val, y_pred))\n\n    return np.mean(f1_scores)\n\n# Create study\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)  # adjust number of trials as needed\n\nprint(\"Best F1 score:\", study.best_value)\nprint(\"Best hyperparameters:\", study.best_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:14:41.165587Z","iopub.execute_input":"2025-10-20T09:14:41.166435Z","iopub.status.idle":"2025-10-20T09:29:05.039955Z","shell.execute_reply.started":"2025-10-20T09:14:41.166409Z","shell.execute_reply":"2025-10-20T09:29:05.039117Z"}},"outputs":[{"name":"stderr","text":"[I 2025-10-20 09:14:41,566] A new study created in memory with name: no-name-a48ec16c-0a8e-4f41-a2bf-bf09d62df491\n[I 2025-10-20 09:14:58,090] Trial 0 finished with value: 0.22037309015809878 and parameters: {'n_estimators': 354, 'max_depth': 7, 'learning_rate': 0.03927104873976637, 'subsample': 0.7906258008049099, 'colsample_bytree': 0.6810623716902173, 'gamma': 1.8855120828360028, 'min_child_weight': 5}. Best is trial 0 with value: 0.22037309015809878.\n[I 2025-10-20 09:15:12,210] Trial 1 finished with value: 0.21858504123636618 and parameters: {'n_estimators': 462, 'max_depth': 5, 'learning_rate': 0.1875713015315927, 'subsample': 0.9995714125616725, 'colsample_bytree': 0.9060666638488061, 'gamma': 2.248481945385719, 'min_child_weight': 10}. Best is trial 0 with value: 0.22037309015809878.\n[I 2025-10-20 09:15:26,571] Trial 2 finished with value: 0.21703825756282194 and parameters: {'n_estimators': 459, 'max_depth': 3, 'learning_rate': 0.1504836851580768, 'subsample': 0.6269233679340785, 'colsample_bytree': 0.7892784586609346, 'gamma': 0.9073217410449258, 'min_child_weight': 8}. Best is trial 0 with value: 0.22037309015809878.\n[I 2025-10-20 09:15:40,250] Trial 3 finished with value: 0.20326830930363443 and parameters: {'n_estimators': 374, 'max_depth': 3, 'learning_rate': 0.011819487594094133, 'subsample': 0.7663719422188026, 'colsample_bytree': 0.7366104281172757, 'gamma': 0.44249668177646695, 'min_child_weight': 4}. Best is trial 0 with value: 0.22037309015809878.\n[I 2025-10-20 09:15:54,282] Trial 4 finished with value: 0.21849510372567155 and parameters: {'n_estimators': 389, 'max_depth': 4, 'learning_rate': 0.18042245415238178, 'subsample': 0.8135783429274395, 'colsample_bytree': 0.9036260504630773, 'gamma': 3.9529717885398425, 'min_child_weight': 6}. Best is trial 0 with value: 0.22037309015809878.\n[I 2025-10-20 09:16:08,845] Trial 5 finished with value: 0.22090790468288307 and parameters: {'n_estimators': 441, 'max_depth': 4, 'learning_rate': 0.2560885188211632, 'subsample': 0.9371032441631292, 'colsample_bytree': 0.6862038102660981, 'gamma': 0.798903079281289, 'min_child_weight': 6}. Best is trial 5 with value: 0.22090790468288307.\n[I 2025-10-20 09:16:20,828] Trial 6 finished with value: 0.21529740234127415 and parameters: {'n_estimators': 108, 'max_depth': 6, 'learning_rate': 0.05498643835205151, 'subsample': 0.8633694634359608, 'colsample_bytree': 0.7961243705309129, 'gamma': 0.07515017470815788, 'min_child_weight': 10}. Best is trial 5 with value: 0.22090790468288307.\n[I 2025-10-20 09:16:34,048] Trial 7 finished with value: 0.2174596809724081 and parameters: {'n_estimators': 233, 'max_depth': 5, 'learning_rate': 0.07471800565737399, 'subsample': 0.9557104231239052, 'colsample_bytree': 0.8901508289607718, 'gamma': 1.5011369649716948, 'min_child_weight': 4}. Best is trial 5 with value: 0.22090790468288307.\n[I 2025-10-20 09:16:52,153] Trial 8 finished with value: 0.21833005175900072 and parameters: {'n_estimators': 337, 'max_depth': 9, 'learning_rate': 0.13471813254236908, 'subsample': 0.7586995094775004, 'colsample_bytree': 0.8167964937519399, 'gamma': 2.3403703502554185, 'min_child_weight': 4}. Best is trial 5 with value: 0.22090790468288307.\n[I 2025-10-20 09:17:06,088] Trial 9 finished with value: 0.21243593535172073 and parameters: {'n_estimators': 425, 'max_depth': 3, 'learning_rate': 0.04117611779334065, 'subsample': 0.9340078274591557, 'colsample_bytree': 0.8287651997424047, 'gamma': 4.878533538865548, 'min_child_weight': 2}. Best is trial 5 with value: 0.22090790468288307.\n[I 2025-10-20 09:17:26,630] Trial 10 finished with value: 0.21978669483915353 and parameters: {'n_estimators': 244, 'max_depth': 10, 'learning_rate': 0.015616322876547623, 'subsample': 0.6630491127731443, 'colsample_bytree': 0.6375892896552598, 'gamma': 3.2512196028847278, 'min_child_weight': 1}. Best is trial 5 with value: 0.22090790468288307.\n[I 2025-10-20 09:17:47,318] Trial 11 finished with value: 0.22173550834298453 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.02478736642180987, 'subsample': 0.876669227431604, 'colsample_bytree': 0.6256831351594321, 'gamma': 1.5744034351204457, 'min_child_weight': 6}. Best is trial 11 with value: 0.22173550834298453.\n[I 2025-10-20 09:18:07,614] Trial 12 finished with value: 0.22166028553122324 and parameters: {'n_estimators': 485, 'max_depth': 8, 'learning_rate': 0.02352385037636811, 'subsample': 0.8854213369010997, 'colsample_bytree': 0.6162881410933625, 'gamma': 1.316326873920905, 'min_child_weight': 7}. Best is trial 11 with value: 0.22173550834298453.\n[I 2025-10-20 09:18:28,244] Trial 13 finished with value: 0.22167552695959233 and parameters: {'n_estimators': 493, 'max_depth': 8, 'learning_rate': 0.022204343058914087, 'subsample': 0.8694869946289122, 'colsample_bytree': 0.6145555358550696, 'gamma': 1.338839581780194, 'min_child_weight': 8}. Best is trial 11 with value: 0.22173550834298453.\n[I 2025-10-20 09:18:48,527] Trial 14 finished with value: 0.22208610419687108 and parameters: {'n_estimators': 496, 'max_depth': 8, 'learning_rate': 0.024530919660537234, 'subsample': 0.8555585277382316, 'colsample_bytree': 0.6734992953912209, 'gamma': 3.138793145181733, 'min_child_weight': 8}. Best is trial 14 with value: 0.22208610419687108.\n[I 2025-10-20 09:19:09,131] Trial 15 finished with value: 0.22311978570095806 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.024649039976582937, 'subsample': 0.6943124090433703, 'colsample_bytree': 0.7080757450744835, 'gamma': 3.082405614360163, 'min_child_weight': 8}. Best is trial 15 with value: 0.22311978570095806.\n[I 2025-10-20 09:19:29,111] Trial 16 finished with value: 0.21740630544289105 and parameters: {'n_estimators': 245, 'max_depth': 10, 'learning_rate': 0.011847461515120323, 'subsample': 0.6982150641656338, 'colsample_bytree': 0.7251233413845031, 'gamma': 3.253907589376505, 'min_child_weight': 9}. Best is trial 15 with value: 0.22311978570095806.\n[I 2025-10-20 09:19:47,261] Trial 17 finished with value: 0.22298157515681058 and parameters: {'n_estimators': 303, 'max_depth': 9, 'learning_rate': 0.03415705323617018, 'subsample': 0.6970604809641623, 'colsample_bytree': 0.7429659871577436, 'gamma': 3.346281642625358, 'min_child_weight': 8}. Best is trial 15 with value: 0.22311978570095806.\n[I 2025-10-20 09:20:04,137] Trial 18 finished with value: 0.2226223177980084 and parameters: {'n_estimators': 295, 'max_depth': 9, 'learning_rate': 0.0952064544097789, 'subsample': 0.7082448999230363, 'colsample_bytree': 0.9664866366412345, 'gamma': 4.110985252140507, 'min_child_weight': 9}. Best is trial 15 with value: 0.22311978570095806.\n[I 2025-10-20 09:20:20,473] Trial 19 finished with value: 0.22241176003203833 and parameters: {'n_estimators': 164, 'max_depth': 10, 'learning_rate': 0.04039878772172547, 'subsample': 0.6016336925778716, 'colsample_bytree': 0.7449890946727037, 'gamma': 2.712794678647617, 'min_child_weight': 7}. Best is trial 15 with value: 0.22311978570095806.\n[I 2025-10-20 09:20:37,800] Trial 20 finished with value: 0.22357577787199423 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.05856355718682789, 'subsample': 0.7154499739875922, 'colsample_bytree': 0.7622853617609749, 'gamma': 3.9952704994928547, 'min_child_weight': 9}. Best is trial 20 with value: 0.22357577787199423.\n[I 2025-10-20 09:20:55,162] Trial 21 finished with value: 0.22383663498236045 and parameters: {'n_estimators': 305, 'max_depth': 9, 'learning_rate': 0.06518447256453899, 'subsample': 0.7150650511571528, 'colsample_bytree': 0.7684750093169381, 'gamma': 3.842840836322439, 'min_child_weight': 9}. Best is trial 21 with value: 0.22383663498236045.\n[I 2025-10-20 09:21:12,120] Trial 22 finished with value: 0.223815482334578 and parameters: {'n_estimators': 283, 'max_depth': 9, 'learning_rate': 0.07056758419004264, 'subsample': 0.7262555571897792, 'colsample_bytree': 0.8570163714505384, 'gamma': 4.068550822376022, 'min_child_weight': 9}. Best is trial 21 with value: 0.22383663498236045.\n[I 2025-10-20 09:21:25,776] Trial 23 finished with value: 0.22112805486751402 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.07985463867479368, 'subsample': 0.746384512645169, 'colsample_bytree': 0.8504246636108077, 'gamma': 4.87792448003407, 'min_child_weight': 10}. Best is trial 21 with value: 0.22383663498236045.\n[I 2025-10-20 09:21:42,440] Trial 24 finished with value: 0.22417451884536704 and parameters: {'n_estimators': 269, 'max_depth': 9, 'learning_rate': 0.05956134463642974, 'subsample': 0.7328156038155015, 'colsample_bytree': 0.7638733144930274, 'gamma': 4.2041004171976155, 'min_child_weight': 9}. Best is trial 24 with value: 0.22417451884536704.\n[I 2025-10-20 09:21:57,360] Trial 25 finished with value: 0.22189466529913768 and parameters: {'n_estimators': 276, 'max_depth': 7, 'learning_rate': 0.1117069698526277, 'subsample': 0.6607950718585327, 'colsample_bytree': 0.848714506866581, 'gamma': 4.462072017860348, 'min_child_weight': 9}. Best is trial 24 with value: 0.22417451884536704.\n[I 2025-10-20 09:22:14,773] Trial 26 finished with value: 0.22455277085188277 and parameters: {'n_estimators': 333, 'max_depth': 9, 'learning_rate': 0.0655564761406778, 'subsample': 0.8160083304625825, 'colsample_bytree': 0.7752714128235971, 'gamma': 3.7868524701804285, 'min_child_weight': 7}. Best is trial 26 with value: 0.22455277085188277.\n[I 2025-10-20 09:22:30,830] Trial 27 finished with value: 0.22246134961108627 and parameters: {'n_estimators': 329, 'max_depth': 8, 'learning_rate': 0.09687280808380669, 'subsample': 0.8085570661083373, 'colsample_bytree': 0.7662486226644954, 'gamma': 3.589506257822216, 'min_child_weight': 7}. Best is trial 26 with value: 0.22455277085188277.\n[I 2025-10-20 09:22:46,280] Trial 28 finished with value: 0.22010155054681216 and parameters: {'n_estimators': 387, 'max_depth': 6, 'learning_rate': 0.05086069002471938, 'subsample': 0.7846392794744831, 'colsample_bytree': 0.7672639340546414, 'gamma': 4.46746598201441, 'min_child_weight': 7}. Best is trial 26 with value: 0.22455277085188277.\n[I 2025-10-20 09:23:02,715] Trial 29 finished with value: 0.2201799513617025 and parameters: {'n_estimators': 352, 'max_depth': 7, 'learning_rate': 0.03558498659196935, 'subsample': 0.8302012278025223, 'colsample_bytree': 0.6698608057508214, 'gamma': 3.621442247138997, 'min_child_weight': 5}. Best is trial 26 with value: 0.22455277085188277.\n[I 2025-10-20 09:23:18,682] Trial 30 finished with value: 0.2222378161414178 and parameters: {'n_estimators': 205, 'max_depth': 9, 'learning_rate': 0.05960506135140579, 'subsample': 0.7830345983316276, 'colsample_bytree': 0.6988154399979932, 'gamma': 2.6512550463513835, 'min_child_weight': 10}. Best is trial 26 with value: 0.22455277085188277.\n[I 2025-10-20 09:23:35,245] Trial 31 finished with value: 0.22440592110815535 and parameters: {'n_estimators': 271, 'max_depth': 9, 'learning_rate': 0.07173714589973476, 'subsample': 0.7324507834782679, 'colsample_bytree': 0.8522059744606065, 'gamma': 4.427886367930177, 'min_child_weight': 9}. Best is trial 26 with value: 0.22455277085188277.\n[I 2025-10-20 09:23:52,177] Trial 32 finished with value: 0.2249137954321147 and parameters: {'n_estimators': 261, 'max_depth': 10, 'learning_rate': 0.08990930067078187, 'subsample': 0.7383453474242968, 'colsample_bytree': 0.9404017578413834, 'gamma': 4.392931728116369, 'min_child_weight': 10}. Best is trial 32 with value: 0.2249137954321147.\n[I 2025-10-20 09:24:08,999] Trial 33 finished with value: 0.22555260098162094 and parameters: {'n_estimators': 266, 'max_depth': 10, 'learning_rate': 0.08686170504177411, 'subsample': 0.7378285012408663, 'colsample_bytree': 0.9749349989419, 'gamma': 4.535240549236887, 'min_child_weight': 10}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:24:24,642] Trial 34 finished with value: 0.2242745371991552 and parameters: {'n_estimators': 208, 'max_depth': 10, 'learning_rate': 0.12150207262274361, 'subsample': 0.6652272699858679, 'colsample_bytree': 0.996683994514608, 'gamma': 4.611861744034262, 'min_child_weight': 10}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:24:39,546] Trial 35 finished with value: 0.22474866710364533 and parameters: {'n_estimators': 176, 'max_depth': 10, 'learning_rate': 0.09073965222917027, 'subsample': 0.7679249131741062, 'colsample_bytree': 0.930605598771601, 'gamma': 4.992029695169809, 'min_child_weight': 10}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:24:53,248] Trial 36 finished with value: 0.22381885826617118 and parameters: {'n_estimators': 150, 'max_depth': 10, 'learning_rate': 0.17315222463019295, 'subsample': 0.7711367569462741, 'colsample_bytree': 0.944895679581157, 'gamma': 4.985225162342215, 'min_child_weight': 10}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:25:06,698] Trial 37 finished with value: 0.21833210570697092 and parameters: {'n_estimators': 154, 'max_depth': 10, 'learning_rate': 0.23425379651686706, 'subsample': 0.8280872244225188, 'colsample_bytree': 0.9352149866592342, 'gamma': 4.566441528771761, 'min_child_weight': 3}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:25:20,970] Trial 38 finished with value: 0.2242183342462054 and parameters: {'n_estimators': 125, 'max_depth': 10, 'learning_rate': 0.09208358484915924, 'subsample': 0.7954057011902615, 'colsample_bytree': 0.9905436991280455, 'gamma': 4.748215549901368, 'min_child_weight': 10}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:25:35,244] Trial 39 finished with value: 0.2226097539989067 and parameters: {'n_estimators': 181, 'max_depth': 10, 'learning_rate': 0.1467652696692875, 'subsample': 0.839403894483719, 'colsample_bytree': 0.9170311673703899, 'gamma': 4.28478644823042, 'min_child_weight': 5}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:25:56,206] Trial 40 finished with value: 0.22527330188268585 and parameters: {'n_estimators': 415, 'max_depth': 10, 'learning_rate': 0.04870284273656522, 'subsample': 0.7530742964242565, 'colsample_bytree': 0.8794568443488522, 'gamma': 3.691811416826469, 'min_child_weight': 10}. Best is trial 33 with value: 0.22555260098162094.\n[I 2025-10-20 09:26:16,596] Trial 41 finished with value: 0.22678774198925095 and parameters: {'n_estimators': 367, 'max_depth': 10, 'learning_rate': 0.048565317749822264, 'subsample': 0.7561242512693747, 'colsample_bytree': 0.878609846001877, 'gamma': 3.5424762662629763, 'min_child_weight': 10}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:26:37,537] Trial 42 finished with value: 0.2248212491710455 and parameters: {'n_estimators': 414, 'max_depth': 10, 'learning_rate': 0.05073088012291384, 'subsample': 0.7704466973731066, 'colsample_bytree': 0.8857544251737984, 'gamma': 3.5202008366343165, 'min_child_weight': 10}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:26:59,004] Trial 43 finished with value: 0.22571077887257304 and parameters: {'n_estimators': 435, 'max_depth': 10, 'learning_rate': 0.04752486514539184, 'subsample': 0.7512874369152537, 'colsample_bytree': 0.8880114992965292, 'gamma': 3.53206570698372, 'min_child_weight': 10}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:27:21,945] Trial 44 finished with value: 0.22519870196011843 and parameters: {'n_estimators': 461, 'max_depth': 10, 'learning_rate': 0.04455401145838106, 'subsample': 0.74928676390984, 'colsample_bytree': 0.8812072685665553, 'gamma': 2.978529010322495, 'min_child_weight': 10}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:27:37,538] Trial 45 finished with value: 0.2192446877009769 and parameters: {'n_estimators': 462, 'max_depth': 5, 'learning_rate': 0.045403845858539106, 'subsample': 0.7418399328263666, 'colsample_bytree': 0.8794377674218873, 'gamma': 2.9568610783958604, 'min_child_weight': 9}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:28:00,761] Trial 46 finished with value: 0.22570789219180626 and parameters: {'n_estimators': 422, 'max_depth': 10, 'learning_rate': 0.030035489075465587, 'subsample': 0.7524121488327323, 'colsample_bytree': 0.8219091500863478, 'gamma': 1.9858449015778858, 'min_child_weight': 10}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:28:21,710] Trial 47 finished with value: 0.22379171371897075 and parameters: {'n_estimators': 415, 'max_depth': 9, 'learning_rate': 0.029031674306989254, 'subsample': 0.7581094901404138, 'colsample_bytree': 0.82351541379994, 'gamma': 1.8458478410379375, 'min_child_weight': 8}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:28:41,355] Trial 48 finished with value: 0.22066449046610606 and parameters: {'n_estimators': 441, 'max_depth': 8, 'learning_rate': 0.019157098694739333, 'subsample': 0.7981674172097893, 'colsample_bytree': 0.8081499883825298, 'gamma': 2.1452954690947412, 'min_child_weight': 10}. Best is trial 41 with value: 0.22678774198925095.\n[I 2025-10-20 09:29:05,035] Trial 49 finished with value: 0.22115862164139938 and parameters: {'n_estimators': 372, 'max_depth': 10, 'learning_rate': 0.033406034799496144, 'subsample': 0.6774221954072959, 'colsample_bytree': 0.9088618755617663, 'gamma': 2.1481278043014576, 'min_child_weight': 1}. Best is trial 41 with value: 0.22678774198925095.\n","output_type":"stream"},{"name":"stdout","text":"Best F1 score: 0.22678774198925095\nBest hyperparameters: {'n_estimators': 367, 'max_depth': 10, 'learning_rate': 0.048565317749822264, 'subsample': 0.7561242512693747, 'colsample_bytree': 0.878609846001877, 'gamma': 3.5424762662629763, 'min_child_weight': 10}\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"**Best F1 score: 0.22678774198925095\nBest hyperparameters: {'n_estimators': 367, 'max_depth': 10, 'learning_rate': 0.048565317749822264, 'subsample': 0.7561242512693747, 'colsample_bytree': 0.878609846001877, 'gamma': 3.5424762662629763, 'min_child_weight': 10}**","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, recall_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\n\n# Assume X_selected and y are your features and target (train dataset)\nX = X_selected.copy()\ny = df_train['readmitted']\n\n# Stratified 5-Fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Best hyperparameters from Optuna\nbest_params = {\n    'n_estimators': 367,\n    'max_depth': 10,\n    'learning_rate': 0.048565317749822264,\n    'subsample': 0.7561242512693747,\n    'colsample_bytree': 0.878609846001877,\n    'gamma': 3.5424762662629763,\n    'min_child_weight': 10\n}\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    # Split fold\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n    # Apply SMOTE only on training fold\n    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n    \n    # Convert back to DataFrame for alignment\n    X_train_res = pd.DataFrame(X_train_res, columns=X_train_fold.columns)\n\n    # Initialize XGBoost with best hyperparameters\n    clf = xgb.XGBClassifier(\n        n_estimators=best_params['n_estimators'],\n        max_depth=best_params['max_depth'],\n        learning_rate=best_params['learning_rate'],\n        subsample=best_params['subsample'],\n        colsample_bytree=best_params['colsample_bytree'],\n        gamma=best_params['gamma'],\n        min_child_weight=best_params['min_child_weight'],\n        eval_metric='logloss',\n        use_label_encoder=False,\n        tree_method='hist',   # Use GPU\n        device='cuda',        # Correct GPU parameter\n        random_state=42\n    )\n\n    # Train on resampled training fold\n    clf.fit(X_train_res, y_train_res)\n\n    # Predict on validation fold\n    y_pred = clf.predict(X_val_fold)\n\n    # Evaluate\n    f1 = f1_score(y_val_fold, y_pred)\n    recall = recall_score(y_val_fold, y_pred)\n    print(f\"Fold {fold}: F1 = {f1:.4f}, Recall = {recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:37:28.963264Z","iopub.execute_input":"2025-10-20T09:37:28.963531Z","iopub.status.idle":"2025-10-20T09:37:45.399173Z","shell.execute_reply.started":"2025-10-20T09:37:28.963516Z","shell.execute_reply":"2025-10-20T09:37:45.398499Z"}},"outputs":[{"name":"stdout","text":"Fold 1: F1 = 0.1663, Recall = 0.1470\nFold 2: F1 = 0.1763, Recall = 0.1541\nFold 3: F1 = 0.1545, Recall = 0.1343\nFold 4: F1 = 0.1769, Recall = 0.1576\nFold 5: F1 = 0.1599, Recall = 0.1413\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Make sure we only use columns that exist in df_train\navailable_features = [col for col in top_features if col in df_train.columns]\nX = df_train[available_features].copy()\ny = df_train['readmitted']\n\nprint(\"Using features for modeling:\", available_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:41:26.390722Z","iopub.execute_input":"2025-10-20T09:41:26.391428Z","iopub.status.idle":"2025-10-20T09:41:26.408795Z","shell.execute_reply.started":"2025-10-20T09:41:26.391399Z","shell.execute_reply":"2025-10-20T09:41:26.407849Z"}},"outputs":[{"name":"stdout","text":"Using features for modeling: ['num_lab_procedures', 'num_medications', 'time_in_hospital', 'number_inpatient', 'age', 'num_procedures', 'discharge_disposition_id', 'number_diagnoses', 'admission_type_id', 'admission_source_id', 'race', 'gender', 'number_outpatient', 'number_emergency', 'change']\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"# BASELINE Evaluation (XGboost)\n","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, recall_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\n\n# X and y from above\n# X = df_train[available_features].copy()\n# y = df_train['readmitted']\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Apply SMOTE to training fold\n    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n    X_train_res = pd.DataFrame(X_train_res, columns=X_train_fold.columns)\n    \n    # Initialize XGBoost classifier with GPU support\n    clf = xgb.XGBClassifier(\n        n_estimators=300,\n        scale_pos_weight=(len(y_train_fold)-sum(y_train_fold))/sum(y_train_fold),\n        tree_method='gpu_hist',  # GPU acceleration\n        use_label_encoder=False,\n        eval_metric='logloss',\n        random_state=42\n    )\n    \n    # Train\n    clf.fit(X_train_res, y_train_res)\n    \n    # Predict on validation fold\n    y_pred = clf.predict(X_val_fold)\n    \n    # Evaluate\n    f1 = f1_score(y_val_fold, y_pred)\n    recall = recall_score(y_val_fold, y_pred)\n    print(f\"Fold {fold}: F1 = {f1:.4f}, Recall = {recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:42:03.954093Z","iopub.execute_input":"2025-10-20T09:42:03.954701Z","iopub.status.idle":"2025-10-20T09:42:10.393313Z","shell.execute_reply.started":"2025-10-20T09:42:03.954677Z","shell.execute_reply":"2025-10-20T09:42:10.392502Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1: F1 = 0.2137, Recall = 0.6840\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 2: F1 = 0.2192, Recall = 0.7001\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 3: F1 = 0.2149, Recall = 0.6794\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 4: F1 = 0.2123, Recall = 0.6746\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 5: F1 = 0.2118, Recall = 0.6809\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:42:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"# BASELINE Evaluation (Lightgbm)","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, recall_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\n\n# Use only your selected features\nfeatures = ['num_lab_procedures', 'num_medications', 'time_in_hospital',\n            'number_inpatient', 'age', 'num_procedures', 'discharge_disposition_id',\n            'number_diagnoses', 'admission_type_id', 'admission_source_id',\n            'race', 'gender', 'number_outpatient', 'number_emergency', 'change']\n\nX = df_train[features]\ny = df_train['readmitted']\n\n# Stratified 5-fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Apply SMOTE on training fold\n    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n    \n    # Initialize LightGBM classifier\n    clf = lgb.LGBMClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        device='gpu'  # Use GPU\n    )\n    \n    # Train on resampled training data\n    clf.fit(X_train_res, y_train_res)\n    \n    # Predict on validation fold\n    y_pred = clf.predict(X_val)\n    \n    # Metrics\n    f1 = f1_score(y_val, y_pred)\n    recall = recall_score(y_val, y_pred)\n    print(f\"Fold {fold}: F1 = {f1:.4f}, Recall = {recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:43:31.603836Z","iopub.execute_input":"2025-10-20T09:43:31.604503Z","iopub.status.idle":"2025-10-20T09:43:55.604544Z","shell.execute_reply.started":"2025-10-20T09:43:31.604481Z","shell.execute_reply":"2025-10-20T09:43:55.603735Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 36163, number of negative: 72327\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 385\n[LightGBM] [Info] Number of data points in the train set: 108490, number of used features: 15\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 11 dense feature groups (1.24 MB) transferred to GPU in 0.002522 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333330 -> initscore=-0.693161\n[LightGBM] [Info] Start training from score -0.693161\nFold 1: F1 = 0.1684, Recall = 0.1483\n[LightGBM] [Info] Number of positive: 36163, number of negative: 72327\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 383\n[LightGBM] [Info] Number of data points in the train set: 108490, number of used features: 15\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 11 dense feature groups (1.24 MB) transferred to GPU in 0.002442 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333330 -> initscore=-0.693161\n[LightGBM] [Info] Start training from score -0.693161\nFold 2: F1 = 0.1843, Recall = 0.1607\n[LightGBM] [Info] Number of positive: 36163, number of negative: 72327\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 378\n[LightGBM] [Info] Number of data points in the train set: 108490, number of used features: 15\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 11 dense feature groups (1.24 MB) transferred to GPU in 0.002284 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333330 -> initscore=-0.693161\n[LightGBM] [Info] Start training from score -0.693161\nFold 3: F1 = 0.1616, Recall = 0.1391\n[LightGBM] [Info] Number of positive: 36163, number of negative: 72327\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 382\n[LightGBM] [Info] Number of data points in the train set: 108490, number of used features: 15\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 11 dense feature groups (1.24 MB) transferred to GPU in 0.002314 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333330 -> initscore=-0.693161\n[LightGBM] [Info] Start training from score -0.693161\nFold 4: F1 = 0.1753, Recall = 0.1519\n[LightGBM] [Info] Number of positive: 36164, number of negative: 72328\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 386\n[LightGBM] [Info] Number of data points in the train set: 108492, number of used features: 15\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 11 dense feature groups (1.24 MB) transferred to GPU in 0.002412 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333333 -> initscore=-0.693147\n[LightGBM] [Info] Start training from score -0.693147\nFold 5: F1 = 0.1562, Recall = 0.1347\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"# Hyper Parameter tunning (lightGBM)","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\n\n# X_selected and y are your train features and target\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ndef objective(trial):\n    params = {\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'device': 'gpu',          # GPU training\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0)\n    }\n\n    f1_scores = []\n\n    for train_idx, val_idx in skf.split(X_selected, y):\n        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]\n        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Apply SMOTE on training fold\n        smote = SMOTE(sampling_strategy=0.5, random_state=42)\n        X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n        X_train_res = pd.DataFrame(X_train_res, columns=X_train_fold.columns)\n\n        model = lgb.LGBMClassifier(**params)\n        model.fit(X_train_res, y_train_res)\n\n        y_pred = model.predict(X_val_fold)\n        f1_scores.append(f1_score(y_val_fold, y_pred))\n\n    return sum(f1_scores)/len(f1_scores)\n\n# Run Optuna study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Best F1 score:\", study.best_value)\nprint(\"Best hyperparameters:\", study.best_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T09:46:25.699990Z","iopub.execute_input":"2025-10-20T09:46:25.700985Z","iopub.status.idle":"2025-10-20T10:16:44.854284Z","shell.execute_reply.started":"2025-10-20T09:46:25.700961Z","shell.execute_reply":"2025-10-20T10:16:44.853682Z"}},"outputs":[{"name":"stderr","text":"[I 2025-10-20 09:46:25,707] A new study created in memory with name: no-name-4bb11c9c-c5e6-4b73-9829-59b698879144\n[I 2025-10-20 09:47:18,161] Trial 0 finished with value: 0.16471021044474612 and parameters: {'n_estimators': 640, 'learning_rate': 0.038466843497743396, 'num_leaves': 70, 'max_depth': 11, 'min_child_samples': 40, 'subsample': 0.8855838799832151, 'colsample_bytree': 0.58374142923337, 'reg_alpha': 4.126469015115222, 'reg_lambda': 4.634034424705844}. Best is trial 0 with value: 0.16471021044474612.\n[I 2025-10-20 09:48:33,882] Trial 1 finished with value: 0.16628989681065723 and parameters: {'n_estimators': 690, 'learning_rate': 0.02839113874041702, 'num_leaves': 134, 'max_depth': 13, 'min_child_samples': 8, 'subsample': 0.7763504180066623, 'colsample_bytree': 0.997836437633665, 'reg_alpha': 4.961971301992701, 'reg_lambda': 1.682403834519432}. Best is trial 1 with value: 0.16628989681065723.\n[I 2025-10-20 09:48:48,147] Trial 2 finished with value: 0.1350179051371097 and parameters: {'n_estimators': 137, 'learning_rate': 0.025361835489002397, 'num_leaves': 144, 'max_depth': 4, 'min_child_samples': 76, 'subsample': 0.7730545149167798, 'colsample_bytree': 0.9756640481957022, 'reg_alpha': 1.66118324829161, 'reg_lambda': 2.858665358386281}. Best is trial 1 with value: 0.16628989681065723.\n[I 2025-10-20 09:49:02,923] Trial 3 finished with value: 0.128649144151957 and parameters: {'n_estimators': 118, 'learning_rate': 0.01957679308919746, 'num_leaves': 60, 'max_depth': 5, 'min_child_samples': 90, 'subsample': 0.8070334207204567, 'colsample_bytree': 0.796533120488309, 'reg_alpha': 4.1148831296456665, 'reg_lambda': 4.062537056284171}. Best is trial 1 with value: 0.16628989681065723.\n[I 2025-10-20 09:49:30,971] Trial 4 finished with value: 0.15842047696981199 and parameters: {'n_estimators': 645, 'learning_rate': 0.01588866384366644, 'num_leaves': 38, 'max_depth': 4, 'min_child_samples': 64, 'subsample': 0.6607190110396743, 'colsample_bytree': 0.7660181696717129, 'reg_alpha': 2.7035880629295894, 'reg_lambda': 0.3092272634432691}. Best is trial 1 with value: 0.16628989681065723.\n[I 2025-10-20 09:50:05,137] Trial 5 finished with value: 0.16492474118975636 and parameters: {'n_estimators': 297, 'learning_rate': 0.03848051006933472, 'num_leaves': 115, 'max_depth': 9, 'min_child_samples': 23, 'subsample': 0.7951726291581416, 'colsample_bytree': 0.6350368968343573, 'reg_alpha': 4.671913577650551, 'reg_lambda': 0.15882423023297731}. Best is trial 1 with value: 0.16628989681065723.\n[I 2025-10-20 09:50:30,553] Trial 6 finished with value: 0.16707158422789573 and parameters: {'n_estimators': 303, 'learning_rate': 0.20834195810641226, 'num_leaves': 106, 'max_depth': 7, 'min_child_samples': 29, 'subsample': 0.8491717164783605, 'colsample_bytree': 0.7074452531716816, 'reg_alpha': 0.23985071787776036, 'reg_lambda': 2.7296340878359615}. Best is trial 6 with value: 0.16707158422789573.\n[I 2025-10-20 09:50:49,307] Trial 7 finished with value: 0.17010304750526634 and parameters: {'n_estimators': 173, 'learning_rate': 0.2368082093070223, 'num_leaves': 94, 'max_depth': 7, 'min_child_samples': 62, 'subsample': 0.76873202205308, 'colsample_bytree': 0.5421016510300507, 'reg_alpha': 0.6054381256849739, 'reg_lambda': 2.9326687158384557}. Best is trial 7 with value: 0.17010304750526634.\n[I 2025-10-20 09:51:13,993] Trial 8 finished with value: 0.15534183874268645 and parameters: {'n_estimators': 681, 'learning_rate': 0.01862502555224969, 'num_leaves': 111, 'max_depth': 3, 'min_child_samples': 34, 'subsample': 0.9169658026587332, 'colsample_bytree': 0.9971469578631698, 'reg_alpha': 0.22492735545424636, 'reg_lambda': 2.7403210498972315}. Best is trial 7 with value: 0.17010304750526634.\n[I 2025-10-20 09:51:42,830] Trial 9 finished with value: 0.17237252236263512 and parameters: {'n_estimators': 648, 'learning_rate': 0.17428435656602448, 'num_leaves': 88, 'max_depth': 5, 'min_child_samples': 53, 'subsample': 0.7172752279836317, 'colsample_bytree': 0.9232808830747699, 'reg_alpha': 0.3384113600786687, 'reg_lambda': 3.9406794243553844}. Best is trial 9 with value: 0.17237252236263512.\n[I 2025-10-20 09:52:21,203] Trial 10 finished with value: 0.16660544920263382 and parameters: {'n_estimators': 991, 'learning_rate': 0.10574328354886492, 'num_leaves': 20, 'max_depth': 14, 'min_child_samples': 97, 'subsample': 0.5178809816757893, 'colsample_bytree': 0.8815742762981726, 'reg_alpha': 1.6449652601225313, 'reg_lambda': 4.175074236834578}. Best is trial 9 with value: 0.17237252236263512.\n[I 2025-10-20 09:52:54,365] Trial 11 finished with value: 0.16622708792308516 and parameters: {'n_estimators': 447, 'learning_rate': 0.29891037518811114, 'num_leaves': 88, 'max_depth': 7, 'min_child_samples': 56, 'subsample': 0.6645603880372468, 'colsample_bytree': 0.5136830669628031, 'reg_alpha': 1.0339540785495607, 'reg_lambda': 3.552526509269792}. Best is trial 9 with value: 0.17237252236263512.\n[I 2025-10-20 09:53:47,192] Trial 12 finished with value: 0.1669078676797417 and parameters: {'n_estimators': 870, 'learning_rate': 0.11570070713796278, 'num_leaves': 84, 'max_depth': 7, 'min_child_samples': 68, 'subsample': 0.9913609978248716, 'colsample_bytree': 0.8513199195346433, 'reg_alpha': 1.0584560295308967, 'reg_lambda': 1.8210880186348408}. Best is trial 9 with value: 0.17237252236263512.\n[I 2025-10-20 09:54:26,018] Trial 13 finished with value: 0.16522688650258516 and parameters: {'n_estimators': 441, 'learning_rate': 0.11151716733192645, 'num_leaves': 91, 'max_depth': 9, 'min_child_samples': 49, 'subsample': 0.6801276423935072, 'colsample_bytree': 0.6691231969975193, 'reg_alpha': 2.521807674092356, 'reg_lambda': 3.580504563693985}. Best is trial 9 with value: 0.17237252236263512.\n[I 2025-10-20 09:54:53,979] Trial 14 finished with value: 0.17059052927718932 and parameters: {'n_estimators': 502, 'learning_rate': 0.18465814807634692, 'num_leaves': 62, 'max_depth': 6, 'min_child_samples': 81, 'subsample': 0.575638212556813, 'colsample_bytree': 0.5149059986361229, 'reg_alpha': 0.017517376553720876, 'reg_lambda': 4.977544615475213}. Best is trial 9 with value: 0.17237252236263512.\n[I 2025-10-20 09:55:29,364] Trial 15 finished with value: 0.17436461391662847 and parameters: {'n_estimators': 825, 'learning_rate': 0.07716930804772155, 'num_leaves': 57, 'max_depth': 5, 'min_child_samples': 81, 'subsample': 0.533291011390941, 'colsample_bytree': 0.9082986852343736, 'reg_alpha': 1.6444753316848963, 'reg_lambda': 4.91553139123847}. Best is trial 15 with value: 0.17436461391662847.\n[I 2025-10-20 09:55:56,358] Trial 16 finished with value: 0.16983599118789944 and parameters: {'n_estimators': 818, 'learning_rate': 0.06961130217923059, 'num_leaves': 45, 'max_depth': 3, 'min_child_samples': 82, 'subsample': 0.5954775279719735, 'colsample_bytree': 0.8996827392887966, 'reg_alpha': 1.9712013613789447, 'reg_lambda': 4.167368102381598}. Best is trial 15 with value: 0.17436461391662847.\n[I 2025-10-20 09:56:42,005] Trial 17 finished with value: 0.16626253607729685 and parameters: {'n_estimators': 808, 'learning_rate': 0.06232385635658819, 'num_leaves': 45, 'max_depth': 11, 'min_child_samples': 42, 'subsample': 0.5069202303834264, 'colsample_bytree': 0.9142228472727593, 'reg_alpha': 3.0498421635309922, 'reg_lambda': 4.77024872173933}. Best is trial 15 with value: 0.17436461391662847.\n[I 2025-10-20 09:57:22,821] Trial 18 finished with value: 0.1728158842714317 and parameters: {'n_estimators': 998, 'learning_rate': 0.14630843527461573, 'num_leaves': 72, 'max_depth': 5, 'min_child_samples': 100, 'subsample': 0.7094964803836069, 'colsample_bytree': 0.8226482743732645, 'reg_alpha': 0.9919661577775939, 'reg_lambda': 3.5249298255596724}. Best is trial 15 with value: 0.17436461391662847.\n[I 2025-10-20 09:58:39,886] Trial 19 finished with value: 0.16430082262595075 and parameters: {'n_estimators': 999, 'learning_rate': 0.010910844089325964, 'num_leaves': 70, 'max_depth': 10, 'min_child_samples': 99, 'subsample': 0.593399006829419, 'colsample_bytree': 0.7863874513827016, 'reg_alpha': 1.0390640851789805, 'reg_lambda': 1.900388094656806}. Best is trial 15 with value: 0.17436461391662847.\n[I 2025-10-20 09:59:18,975] Trial 20 finished with value: 0.17356166447980392 and parameters: {'n_estimators': 905, 'learning_rate': 0.07691129335252994, 'num_leaves': 27, 'max_depth': 5, 'min_child_samples': 89, 'subsample': 0.7105506903769786, 'colsample_bytree': 0.8340220158781643, 'reg_alpha': 3.241448980730194, 'reg_lambda': 3.4186166130727775}. Best is trial 15 with value: 0.17436461391662847.\n[I 2025-10-20 09:59:57,062] Trial 21 finished with value: 0.17441998568232114 and parameters: {'n_estimators': 918, 'learning_rate': 0.08369079038634761, 'num_leaves': 22, 'max_depth': 5, 'min_child_samples': 89, 'subsample': 0.7089966345201377, 'colsample_bytree': 0.8278021530817419, 'reg_alpha': 3.383197912477133, 'reg_lambda': 3.423604992077294}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:00:34,244] Trial 22 finished with value: 0.17143653589096058 and parameters: {'n_estimators': 883, 'learning_rate': 0.0678226244624946, 'num_leaves': 21, 'max_depth': 6, 'min_child_samples': 88, 'subsample': 0.6355974690865502, 'colsample_bytree': 0.8452383077713661, 'reg_alpha': 3.333053412505178, 'reg_lambda': 2.1883003440044315}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:01:04,594] Trial 23 finished with value: 0.17272517659838352 and parameters: {'n_estimators': 759, 'learning_rate': 0.08531892601157871, 'num_leaves': 33, 'max_depth': 4, 'min_child_samples': 74, 'subsample': 0.5508461508289735, 'colsample_bytree': 0.7352116152085264, 'reg_alpha': 3.574327059472779, 'reg_lambda': 1.2907164529175708}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:01:49,675] Trial 24 finished with value: 0.17109477538732684 and parameters: {'n_estimators': 915, 'learning_rate': 0.04141441198292496, 'num_leaves': 32, 'max_depth': 8, 'min_child_samples': 87, 'subsample': 0.7228596484663528, 'colsample_bytree': 0.951404933875264, 'reg_alpha': 2.1560243171918163, 'reg_lambda': 3.2542379630941385}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:02:31,183] Trial 25 finished with value: 0.17215057542080853 and parameters: {'n_estimators': 767, 'learning_rate': 0.046336403826085955, 'num_leaves': 48, 'max_depth': 6, 'min_child_samples': 72, 'subsample': 0.6291553050057069, 'colsample_bytree': 0.8728001159499275, 'reg_alpha': 3.767750645886252, 'reg_lambda': 4.577345046122482}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:03:01,072] Trial 26 finished with value: 0.17224043897467176 and parameters: {'n_estimators': 943, 'learning_rate': 0.08709520940009745, 'num_leaves': 54, 'max_depth': 3, 'min_child_samples': 81, 'subsample': 0.8350529031343619, 'colsample_bytree': 0.8193532348355789, 'reg_alpha': 2.963667155576456, 'reg_lambda': 2.352222100256556}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:03:38,398] Trial 27 finished with value: 0.17042278462776075 and parameters: {'n_estimators': 841, 'learning_rate': 0.05201714630530946, 'num_leaves': 29, 'max_depth': 5, 'min_child_samples': 92, 'subsample': 0.5449922256964687, 'colsample_bytree': 0.730643850116486, 'reg_alpha': 2.135102575546588, 'reg_lambda': 3.1935927143318104}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:04:18,315] Trial 28 finished with value: 0.1703102064074417 and parameters: {'n_estimators': 755, 'learning_rate': 0.0882932316469887, 'num_leaves': 39, 'max_depth': 8, 'min_child_samples': 93, 'subsample': 0.7389102210645199, 'colsample_bytree': 0.9425526400260178, 'reg_alpha': 3.18709769134895, 'reg_lambda': 4.440517259035871}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:04:46,188] Trial 29 finished with value: 0.16550935225847704 and parameters: {'n_estimators': 551, 'learning_rate': 0.1388901763580565, 'num_leaves': 26, 'max_depth': 12, 'min_child_samples': 83, 'subsample': 0.870618546710161, 'colsample_bytree': 0.8582263415536763, 'reg_alpha': 4.199986179056778, 'reg_lambda': 1.1247293547239727}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:05:11,647] Trial 30 finished with value: 0.17104284200706357 and parameters: {'n_estimators': 589, 'learning_rate': 0.05935069972817876, 'num_leaves': 71, 'max_depth': 4, 'min_child_samples': 76, 'subsample': 0.9498851045677892, 'colsample_bytree': 0.8164134177303288, 'reg_alpha': 2.7470708243945063, 'reg_lambda': 3.798873873192956}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:05:51,723] Trial 31 finished with value: 0.1728811689465966 and parameters: {'n_estimators': 958, 'learning_rate': 0.14529800479679278, 'num_leaves': 75, 'max_depth': 5, 'min_child_samples': 100, 'subsample': 0.6993990188971242, 'colsample_bytree': 0.8277204794142894, 'reg_alpha': 1.2745086815639546, 'reg_lambda': 3.344417078326006}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:06:37,898] Trial 32 finished with value: 0.16704794108853926 and parameters: {'n_estimators': 908, 'learning_rate': 0.14476105620698657, 'num_leaves': 55, 'max_depth': 6, 'min_child_samples': 94, 'subsample': 0.6900074801389908, 'colsample_bytree': 0.780708185642372, 'reg_alpha': 1.494527313501267, 'reg_lambda': 3.0924622973232463}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:07:19,474] Trial 33 finished with value: 0.1711424300171116 and parameters: {'n_estimators': 959, 'learning_rate': 0.029845642163713968, 'num_leaves': 80, 'max_depth': 5, 'min_child_samples': 86, 'subsample': 0.6306393549316456, 'colsample_bytree': 0.9017227729343785, 'reg_alpha': 1.3809863174102883, 'reg_lambda': 2.538244043587864}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:07:47,679] Trial 34 finished with value: 0.17353520934522174 and parameters: {'n_estimators': 712, 'learning_rate': 0.07441372989983706, 'num_leaves': 61, 'max_depth': 4, 'min_child_samples': 13, 'subsample': 0.7640219202422152, 'colsample_bytree': 0.9557077202341273, 'reg_alpha': 1.935316569565163, 'reg_lambda': 3.401979879612141}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:08:13,133] Trial 35 finished with value: 0.16889366770011277 and parameters: {'n_estimators': 746, 'learning_rate': 0.08104152864647289, 'num_leaves': 60, 'max_depth': 3, 'min_child_samples': 17, 'subsample': 0.7509307686937299, 'colsample_bytree': 0.9600526151274319, 'reg_alpha': 3.882881911982767, 'reg_lambda': 4.327569571879083}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:08:41,167] Trial 36 finished with value: 0.1704936276877807 and parameters: {'n_estimators': 693, 'learning_rate': 0.05033870679826317, 'num_leaves': 39, 'max_depth': 4, 'min_child_samples': 8, 'subsample': 0.8252112937423242, 'colsample_bytree': 0.9997302287758374, 'reg_alpha': 1.9101848915787325, 'reg_lambda': 3.840666351948525}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:09:12,601] Trial 37 finished with value: 0.1728452809338998 and parameters: {'n_estimators': 830, 'learning_rate': 0.07510939699842238, 'num_leaves': 150, 'max_depth': 4, 'min_child_samples': 61, 'subsample': 0.7955315504527383, 'colsample_bytree': 0.9270331507163241, 'reg_alpha': 2.4122916709324045, 'reg_lambda': 4.821245657852844}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:09:55,671] Trial 38 finished with value: 0.16428332148956573 and parameters: {'n_estimators': 714, 'learning_rate': 0.10232473762708708, 'num_leaves': 52, 'max_depth': 15, 'min_child_samples': 44, 'subsample': 0.7837372624459424, 'colsample_bytree': 0.8848547503025672, 'reg_alpha': 3.553891934587572, 'reg_lambda': 3.716402583097611}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:10:43,951] Trial 39 finished with value: 0.1703610978393318 and parameters: {'n_estimators': 863, 'learning_rate': 0.03346481854239312, 'num_leaves': 63, 'max_depth': 6, 'min_child_samples': 16, 'subsample': 0.7525972640094521, 'colsample_bytree': 0.7670788116614583, 'reg_alpha': 4.265890438929401, 'reg_lambda': 2.9353056417719348}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:11:49,551] Trial 40 finished with value: 0.1695277273546267 and parameters: {'n_estimators': 788, 'learning_rate': 0.02452150961310042, 'num_leaves': 125, 'max_depth': 8, 'min_child_samples': 32, 'subsample': 0.8897731904793706, 'colsample_bytree': 0.980056283802497, 'reg_alpha': 2.764242579329781, 'reg_lambda': 2.5910039878076656}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:12:29,094] Trial 41 finished with value: 0.17028322280692856 and parameters: {'n_estimators': 932, 'learning_rate': 0.1317943884202102, 'num_leaves': 100, 'max_depth': 5, 'min_child_samples': 95, 'subsample': 0.6955151911533649, 'colsample_bytree': 0.8251154027286204, 'reg_alpha': 1.8251703757757678, 'reg_lambda': 3.328127446226007}. Best is trial 21 with value: 0.17441998568232114.\n[I 2025-10-20 10:13:00,827] Trial 42 finished with value: 0.17545255556207004 and parameters: {'n_estimators': 875, 'learning_rate': 0.09884213396280735, 'num_leaves': 77, 'max_depth': 4, 'min_child_samples': 77, 'subsample': 0.6573004980884636, 'colsample_bytree': 0.8560838492917963, 'reg_alpha': 1.297857471332764, 'reg_lambda': 3.4131333456386432}. Best is trial 42 with value: 0.17545255556207004.\n[I 2025-10-20 10:13:33,258] Trial 43 finished with value: 0.17512922095296746 and parameters: {'n_estimators': 890, 'learning_rate': 0.09634346075292026, 'num_leaves': 64, 'max_depth': 4, 'min_child_samples': 67, 'subsample': 0.6582042977476417, 'colsample_bytree': 0.8715707125732536, 'reg_alpha': 1.7157180987721323, 'reg_lambda': 4.025073513061214}. Best is trial 42 with value: 0.17545255556207004.\n[I 2025-10-20 10:14:01,569] Trial 44 finished with value: 0.16946399043407145 and parameters: {'n_estimators': 885, 'learning_rate': 0.09988361754953413, 'num_leaves': 82, 'max_depth': 3, 'min_child_samples': 69, 'subsample': 0.6579923563181529, 'colsample_bytree': 0.7951565027706139, 'reg_alpha': 0.7472349389857681, 'reg_lambda': 4.01590317745084}. Best is trial 42 with value: 0.17545255556207004.\n[I 2025-10-20 10:14:37,601] Trial 45 finished with value: 0.16881421658717005 and parameters: {'n_estimators': 844, 'learning_rate': 0.09120363547689098, 'num_leaves': 24, 'max_depth': 7, 'min_child_samples': 77, 'subsample': 0.6576313283098341, 'colsample_bytree': 0.8651688682132079, 'reg_alpha': 2.3623622919285463, 'reg_lambda': 4.310636503395193}. Best is trial 42 with value: 0.17545255556207004.\n[I 2025-10-20 10:15:18,102] Trial 46 finished with value: 0.1733364324542336 and parameters: {'n_estimators': 905, 'learning_rate': 0.05830181559640885, 'num_leaves': 78, 'max_depth': 5, 'min_child_samples': 66, 'subsample': 0.616929200050158, 'colsample_bytree': 0.8453115636087255, 'reg_alpha': 4.493440377838422, 'reg_lambda': 4.581530617683029}. Best is trial 42 with value: 0.17545255556207004.\n[I 2025-10-20 10:15:34,840] Trial 47 finished with value: 0.16780965334837955 and parameters: {'n_estimators': 312, 'learning_rate': 0.11866935143162953, 'num_leaves': 99, 'max_depth': 3, 'min_child_samples': 61, 'subsample': 0.7355120098618152, 'colsample_bytree': 0.8881646476947873, 'reg_alpha': 4.958265190161784, 'reg_lambda': 3.984214141617193}. Best is trial 42 with value: 0.17545255556207004.\n[I 2025-10-20 10:16:09,105] Trial 48 finished with value: 0.17458119118664842 and parameters: {'n_estimators': 961, 'learning_rate': 0.06696725283703829, 'num_leaves': 36, 'max_depth': 4, 'min_child_samples': 72, 'subsample': 0.5384244562029966, 'colsample_bytree': 0.9264741051634633, 'reg_alpha': 1.645047648058669, 'reg_lambda': 4.98712846670349}. Best is trial 42 with value: 0.17545255556207004.\n[I 2025-10-20 10:16:44,850] Trial 49 finished with value: 0.17291902728541717 and parameters: {'n_estimators': 970, 'learning_rate': 0.06414973395044171, 'num_leaves': 67, 'max_depth': 4, 'min_child_samples': 70, 'subsample': 0.5309230616547713, 'colsample_bytree': 0.5848489271066485, 'reg_alpha': 1.5588617672995662, 'reg_lambda': 4.811627406533922}. Best is trial 42 with value: 0.17545255556207004.\n","output_type":"stream"},{"name":"stdout","text":"Best F1 score: 0.17545255556207004\nBest hyperparameters: {'n_estimators': 875, 'learning_rate': 0.09884213396280735, 'num_leaves': 77, 'max_depth': 4, 'min_child_samples': 77, 'subsample': 0.6573004980884636, 'colsample_bytree': 0.8560838492917963, 'reg_alpha': 1.297857471332764, 'reg_lambda': 3.4131333456386432}\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"**After , now using tunned Parameters**","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, recall_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\n\n# ✅ Your best Optuna hyperparameters\nbest_params = {\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'device': 'gpu',  # GPU enabled\n    'metric': 'binary_logloss',\n    'verbosity': -1,\n    'n_estimators': 875,\n    'learning_rate': 0.09884213396280735,\n    'num_leaves': 77,\n    'max_depth': 4,\n    'min_child_samples': 77,\n    'subsample': 0.6573004980884636,\n    'colsample_bytree': 0.8560838492917963,\n    'reg_alpha': 1.297857471332764,\n    'reg_lambda': 3.4131333456386432\n}\n\n# 5-Fold Stratified CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nf1_scores, recall_scores = [], []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y), 1):\n    X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n    # Apply SMOTE on training data\n    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n    X_train_res = pd.DataFrame(X_train_res, columns=X_train_fold.columns)\n\n    # Train LGBM model\n    model = lgb.LGBMClassifier(**best_params)\n    model.fit(X_train_res, y_train_res)\n\n    # Predict on validation set\n    y_pred = model.predict(X_val_fold)\n\n    # Evaluate\n    f1 = f1_score(y_val_fold, y_pred)\n    recall = recall_score(y_val_fold, y_pred)\n    f1_scores.append(f1)\n    recall_scores.append(recall)\n\n    print(f\"Fold {fold}: F1 = {f1:.4f}, Recall = {recall:.4f}\")\n\n# Average results\nprint(\"\\nAverage F1:\", sum(f1_scores)/len(f1_scores))\nprint(\"Average Recall:\", sum(recall_scores)/len(recall_scores))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T10:18:30.179809Z","iopub.execute_input":"2025-10-20T10:18:30.180137Z","iopub.status.idle":"2025-10-20T10:19:02.254623Z","shell.execute_reply.started":"2025-10-20T10:18:30.180113Z","shell.execute_reply":"2025-10-20T10:19:02.253886Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\nFold 1: F1 = 0.1773, Recall = 0.1633\nFold 2: F1 = 0.1840, Recall = 0.1682\nFold 3: F1 = 0.1693, Recall = 0.1519\nFold 4: F1 = 0.1817, Recall = 0.1664\nFold 5: F1 = 0.1650, Recall = 0.1510\n\nAverage F1: 0.17545255556207004\nAverage Recall: 0.16016607128459884\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"# ENSEMBLING","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, recall_score\nimport numpy as np\n\n# Define base models with best hyperparameters\nxgb_model = xgb.XGBClassifier(\n    n_estimators=367,\n    max_depth=10,\n    learning_rate=0.048565317749822264,\n    subsample=0.7561242512693747,\n    colsample_bytree=0.878609846001877,\n    gamma=3.5424762662629763,\n    min_child_weight=10,\n    random_state=42,\n    tree_method=\"hist\",\n    device=\"cuda\"\n)\n\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=875,\n    learning_rate=0.09884213396280735,\n    num_leaves=77,\n    max_depth=4,\n    min_child_samples=77,\n    subsample=0.6573004980884636,\n    colsample_bytree=0.8560838492917963,\n    reg_alpha=1.297857471332764,\n    reg_lambda=3.4131333456386432,\n    device=\"gpu\",\n    random_state=42\n)\n\n# Meta-model\nmeta_model = LogisticRegression(max_iter=500, solver='lbfgs')\n\n# Prepare cross-validation\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nf1_scores, recall_scores = [], []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_selected, y), 1):\n    X_train, X_val = X_selected.iloc[train_idx], X_selected.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n    # Train base models\n    xgb_model.fit(X_train, y_train)\n    lgb_model.fit(X_train, y_train)\n\n    # Predict probabilities\n    xgb_pred = xgb_model.predict_proba(X_val)[:, 1]\n    lgb_pred = lgb_model.predict_proba(X_val)[:, 1]\n\n    # Stack predictions as meta-features\n    meta_X_train = np.vstack((xgb_pred, lgb_pred)).T\n\n    # Fit meta-model\n    meta_model.fit(meta_X_train, y_val)\n\n    # Final predictions\n    meta_preds = meta_model.predict(meta_X_train)\n\n    f1 = f1_score(y_val, meta_preds)\n    recall = recall_score(y_val, meta_preds)\n    f1_scores.append(f1)\n    recall_scores.append(recall)\n\n    print(f\"Fold {fold}: F1 = {f1:.4f}, Recall = {recall:.4f}\")\n\nprint(f\"\\nAverage F1: {np.mean(f1_scores):.4f}\")\nprint(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T10:53:47.028995Z","iopub.execute_input":"2025-10-20T10:53:47.029919Z","iopub.status.idle":"2025-10-20T10:54:12.284521Z","shell.execute_reply.started":"2025-10-20T10:53:47.029865Z","shell.execute_reply":"2025-10-20T10:54:12.283772Z"}},"outputs":[{"name":"stdout","text":"Fold 1: F1 = 0.0507, Recall = 0.0268\nFold 2: F1 = 0.0620, Recall = 0.0330\nFold 3: F1 = 0.0427, Recall = 0.0225\nFold 4: F1 = 0.0523, Recall = 0.0277\nFold 5: F1 = 0.0362, Recall = 0.0189\n\nAverage F1: 0.0488\nAverage Recall: 0.0258\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}